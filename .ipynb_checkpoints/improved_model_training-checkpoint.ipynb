{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5093f839",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b71ec",
   "metadata": {},
   "source": [
    "### 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2884357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import time\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ee289",
   "metadata": {},
   "source": [
    "### 2. Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a520f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n",
      "✓ NLTK stopwords downloaded successfully\n",
      "Loading dataset...\n",
      "✓ Dataset loaded successfully with 225002 rows\n",
      "\n",
      "Initial dataset information:\n",
      "Shape: (225002, 5)\n",
      "Columns: ['userName', 'content', 'score', 'at', 'appVersion']\n",
      "\n",
      "Sample data:\n",
      "                  userName  \\\n",
      "0                Yuga Edit   \n",
      "1                 ff burik   \n",
      "2  Anisa Suci Rahmayuliani   \n",
      "\n",
      "                                                                                               content  \\\n",
      "0                                                                              akun gopay saya di blok   \n",
      "1                                            Lambat sekali sekarang ini bosssku apk gojek gk kaya dulu   \n",
      "2  Kenapa sih dari kemarin sy buka aplikasi gojek malah keluar sendiri terus Saya kasih bintang 2 d...   \n",
      "\n",
      "   score                   at appVersion  \n",
      "0      1  2022-01-21 10:52:12      4.9.3  \n",
      "1      3  2021-11-30 15:40:38      4.9.3  \n",
      "2      4  2021-11-29 22:58:12      4.9.3  \n",
      "\n",
      "1. Performing sentiment labeling...\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positif    161371\n",
      "negatif     54171\n",
      "netral       9460\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Cleaning text...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Apply text cleaning with appropriate progress tracking\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_progress_apply:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mcleaned_content\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing text cleaning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bimag\\OneDrive\\文档\\PK UAS\\.venv\\Lib\\site-packages\\tqdm\\std.py:885\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n\u001b[32m    883\u001b[39m     deprecated_t[\u001b[32m0\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m     t = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m0\u001b[39m:\n\u001b[32m    888\u001b[39m     \u001b[38;5;66;03m# *args intentionally not supported (see #244, #299)\u001b[39;00m\n\u001b[32m    889\u001b[39m     TqdmDeprecationWarning(\n\u001b[32m    890\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExcept func, normal arguments are intentionally\u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m    891\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m not supported by\u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m    892\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `(DataFrame|Series|GroupBy).progress_apply`.\u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m    893\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Use keyword arguments instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    894\u001b[39m         fp_write=\u001b[38;5;28mgetattr\u001b[39m(t.fp, \u001b[33m'\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m'\u001b[39m, sys.stderr.write))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bimag\\OneDrive\\文档\\PK UAS\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bimag\\OneDrive\\文档\\PK UAS\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Set pandas display options for better visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to create a simple progress indicator as fallback\n",
    "def simple_progress(iterable, desc=None):\n",
    "    \"\"\"Simple progress indicator if tqdm is not available\"\"\"\n",
    "    if desc:\n",
    "        print(f\"{desc}...\")\n",
    "    return iterable\n",
    "\n",
    "# Try to use tqdm, fall back to simple progress if not available\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm as tqdm_std\n",
    "    tqdm_available = True\n",
    "    \n",
    "    # Check if we're in a notebook environment\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if get_ipython() is not None and 'IPKernelApp' in get_ipython().config:\n",
    "            # We're in a notebook, try to use pandas progress_apply\n",
    "            try:\n",
    "                tqdm.pandas()\n",
    "                use_progress_apply = True\n",
    "            except:\n",
    "                use_progress_apply = False\n",
    "        else:\n",
    "            use_progress_apply = False\n",
    "    except:\n",
    "        use_progress_apply = False\n",
    "        \n",
    "except ImportError:\n",
    "    tqdm_available = False\n",
    "    use_progress_apply = False\n",
    "    tqdm = simple_progress\n",
    "    tqdm_std = simple_progress\n",
    "\n",
    "# use_progress_apply = False  # Default to no progress apply if tqdm is not available\n",
    "# tqdm_available = False  # Disable tqdm if not available\n",
    "\n",
    "# Download required NLTK resources\n",
    "print(\"Downloading NLTK resources...\")\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"✓ NLTK stopwords downloaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    print(\"Loading dataset...\")\n",
    "    file_path = \"DATA/GojekAppReview_1.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✓ Dataset loaded successfully with {len(df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display initial dataset info\n",
    "print(\"\\nInitial dataset information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# 1. SENTIMENT LABELING\n",
    "print(\"\\n1. Performing sentiment labeling...\")\n",
    "\n",
    "def label_sentiment(score):\n",
    "    \"\"\"Label reviews based on their score\"\"\"\n",
    "    if score >= 4:\n",
    "        return 'positif'\n",
    "    elif score == 3:\n",
    "        return 'netral'\n",
    "    else:\n",
    "        return 'negatif'\n",
    "\n",
    "df['sentiment'] = df['score'].apply(label_sentiment)\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "\n",
    "# 2. TEXT CLEANING\n",
    "print(\"\\n2. Cleaning text...\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Replace emojis with empty string\n",
    "    try:\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "    except:\n",
    "        # In case emoji module has issues\n",
    "        pass\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "# Apply text cleaning with appropriate progress tracking\n",
    "if use_progress_apply:\n",
    "    df['cleaned_content'] = df['content'].progress_apply(clean_text)\n",
    "else:\n",
    "    print(\"Processing text cleaning...\")\n",
    "    df['cleaned_content'] = [clean_text(text) for text in tqdm(df['content'])]\n",
    "\n",
    "# 3. WORD NORMALIZATION\n",
    "print(\"\\n3. Normalizing text...\")\n",
    "\n",
    "# Normalization dictionary\n",
    "normalization_dict = {\n",
    "    # Basic conversions\n",
    "    \"gk\": \"gak\", \"ga\": \"gak\", \"tdk\": \"tidak\", \"bgt\": \"banget\", \"dr\": \"dari\",\n",
    "    \"udh\": \"sudah\", \"jg\": \"juga\", \"aja\": \"saja\", \"sy\": \"saya\", \"trs\": \"terus\",\n",
    "    \"ngga\": \"tidak\", \"nggak\": \"tidak\", \"bkin\": \"bikin\", \"blm\": \"belum\",\n",
    "    \"sm\": \"sama\", \"tp\": \"tapi\", \"dgn\": \"dengan\", \"krn\": \"karena\",\n",
    "    \n",
    "    # Additional common Indonesian slang words\n",
    "    \"yg\": \"yang\", \"utk\": \"untuk\", \"dg\": \"dengan\", \"klo\": \"kalau\", \"kok\": \"kok\",\n",
    "    \"gw\": \"saya\", \"gue\": \"saya\", \"lu\": \"kamu\", \"kyk\": \"seperti\", \"gmn\": \"bagaimana\",\n",
    "    \"sih\": \"sih\", \"deh\": \"deh\", \"dpt\": \"dapat\", \"bs\": \"bisa\", \"sdh\": \"sudah\",\n",
    "    \"ttg\": \"tentang\", \"dlm\": \"dalam\", \"kl\": \"kalau\", \"km\": \"kamu\", \"hrs\": \"harus\",\n",
    "    \"mk\": \"maka\", \"scr\": \"secara\", \"spy\": \"supaya\", \"bnyk\": \"banyak\", \"slh\": \"salah\",\n",
    "    \"krna\": \"karena\", \"mw\": \"mau\", \"pk\": \"pakai\", \"pke\": \"pakai\", \"tq\": \"terima kasih\",\n",
    "    \"thx\": \"terima kasih\", \"gpp\": \"tidak apa-apa\", \"gampng\": \"gampang\", \"bwt\": \"buat\",\n",
    "    \"skrng\": \"sekarang\", \"skrg\": \"sekarang\", \"msh\": \"masih\", \"bnr\": \"benar\",\n",
    "    \"trims\": \"terima kasih\", \"gk\": \"tidak\", \"gak\": \"tidak\", \"udah\": \"sudah\",\n",
    "    \"pgen\": \"ingin\", \"pgn\": \"ingin\", \"kyk\": \"seperti\", \"gitu\": \"begitu\",\n",
    "    \"gini\": \"begini\", \"gmana\": \"bagaimana\", \"gimana\": \"bagaimana\", \"gt\": \"begitu\",\n",
    "    \"yah\": \"ya\", \"karna\": \"karena\", \"dri\": \"dari\", \"tdk\": \"tidak\",\n",
    "    \"knp\": \"kenapa\", \"kpn\": \"kapan\", \"nih\": \"ini\", \"spt\": \"seperti\",\n",
    "    \"ntaps\": \"mantap\", \"mantul\": \"mantap\", \"mantap\": \"mantap\"\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Indonesian informal words\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    normalized_words = [normalization_dict.get(word, word) for word in words]\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Apply normalization with appropriate progress tracking\n",
    "if use_progress_apply:\n",
    "    df['normalized_content'] = df['cleaned_content'].progress_apply(normalize_text)\n",
    "else:\n",
    "    print(\"Processing text normalization...\")\n",
    "    df['normalized_content'] = [normalize_text(text) for text in tqdm(df['cleaned_content'])]\n",
    "\n",
    "# 4. TOKENIZATION AND STOPWORD REMOVAL\n",
    "print(\"\\n4. Tokenizing and removing stopwords...\")\n",
    "\n",
    "# Get Indonesian stopwords\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = {\"yang\", \"dengan\", \"pada\", \"untuk\", \"dari\", \"di\", \"ke\", \"dan\", \"atau\", \"ini\", \"itu\"}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    \"\"\"Tokenize text and remove stopwords\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Simple tokenization by splitting on spaces\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply tokenization with appropriate progress tracking\n",
    "if use_progress_apply:\n",
    "    df['tokenized_content'] = df['normalized_content'].progress_apply(tokenize_and_remove_stopwords)\n",
    "else:\n",
    "    print(\"Processing tokenization and stopword removal...\")\n",
    "    df['tokenized_content'] = [tokenize_and_remove_stopwords(text) for text in tqdm(df['normalized_content'])]\n",
    "\n",
    "# 5. STEMMING\n",
    "print(\"\\n5. Stemming text...\")\n",
    "\n",
    "# Create Sastrawi stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply stemming to text using Sastrawi stemmer\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    return stemmer.stem(text)\n",
    "\n",
    "# Apply stemming with appropriate progress tracking\n",
    "if use_progress_apply:\n",
    "    df['stemmed_content'] = df['tokenized_content'].progress_apply(stem_text)\n",
    "else:\n",
    "    print(\"Processing stemming...\")\n",
    "    df['stemmed_content'] = [stem_text(text) for text in tqdm(df['tokenized_content'])]\n",
    "\n",
    "# SAVE PROCESSED DATASET\n",
    "print(\"\\nSaving processed dataset...\")\n",
    "output_file_path = \"DATA/GojekAppReview_Processed.csv\"\n",
    "\n",
    "try:\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"✓ Processed dataset saved to {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving dataset: {e}\")\n",
    "\n",
    "# DISPLAY FINAL RESULTS\n",
    "print(\"\\nFinal dataset preview:\")\n",
    "final_columns = ['userName', 'content', 'score', 'at', 'appVersion', 'sentiment', \n",
    "                 'cleaned_content', 'normalized_content', 'tokenized_content', 'stemmed_content']\n",
    "print(df[final_columns].head())\n",
    "\n",
    "# Display a sample of processing steps for a single review\n",
    "print(\"\\nProcessing steps example for a single review:\")\n",
    "sample_idx = 0\n",
    "print(f\"Original:    {df.iloc[sample_idx]['content']}\")\n",
    "print(f\"Cleaned:     {df.iloc[sample_idx]['cleaned_content']}\")\n",
    "print(f\"Normalized:  {df.iloc[sample_idx]['normalized_content']}\")\n",
    "print(f\"Tokenized:   {df.iloc[sample_idx]['tokenized_content']}\")\n",
    "print(f\"Stemmed:     {df.iloc[sample_idx]['stemmed_content']}\")\n",
    "print(f\"Sentiment:   {df.iloc[sample_idx]['sentiment']}\")\n",
    "\n",
    "# Calculate and display processing time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Optional: Data visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='sentiment', data=df, palette='viridis')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['score'].value_counts().sort_index().plot(kind='bar', color='teal')\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Word clouds for each sentiment\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    for i, sentiment in enumerate(['positif', 'netral', 'negatif']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        # Combine all stemmed content for the sentiment\n",
    "        text = ' '.join(df[df['sentiment'] == sentiment]['stemmed_content'].dropna())\n",
    "        \n",
    "        if text:\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                                  max_words=100, contour_width=3, contour_color='steelblue').generate(text)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.title(f'Word Cloud - {sentiment.capitalize()} Reviews')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Note: WordCloud not installed. To visualize word clouds, install with: pip install wordcloud\")\n",
    "\n",
    "\n",
    "    # PREPARE FINAL DATASET FOR SAVING\n",
    "print(\"\\nPreparing final dataset for saving...\")\n",
    "\n",
    "# Rename sentiment column to match requested format\n",
    "df = df.rename(columns={'sentiment': 'sentimenLabel'})\n",
    "\n",
    "# Select columns for the final output as requested\n",
    "final_output_columns = ['userName', 'content', 'score', 'at', 'appVersion', 'sentimenLabel']\n",
    "final_df = df[final_output_columns].copy()\n",
    "\n",
    "# Create 'data' directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    print(\"✓ Created 'data' directory\")\n",
    "\n",
    "# Add timestamp to filename to avoid overwriting\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file_path = f\"data/GojekAppReview_Processed_{timestamp}.csv\"\n",
    "\n",
    "# Also save a copy with a standard name for easy reference\n",
    "standard_output_path = \"data/GojekAppReview_Processed.csv\"\n",
    "\n",
    "# SAVE PROCESSED DATASET\n",
    "print(\"\\nSaving processed dataset...\")\n",
    "\n",
    "try:\n",
    "    # Save timestamped version\n",
    "    final_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"✓ Processed dataset saved with timestamp: {output_file_path}\")\n",
    "    \n",
    "    # Save standard version\n",
    "    final_df.to_csv(standard_output_path, index=False)\n",
    "    print(f\"✓ Processed dataset also saved as: {standard_output_path}\")\n",
    "    \n",
    "    # Save full processed version with all preprocessing columns\n",
    "    full_output_path = \"data/GojekAppReview_Processed_Full.csv\"\n",
    "    df.to_csv(full_output_path, index=False)\n",
    "    print(f\"✓ Complete processed dataset (with all preprocessing steps) saved as: {full_output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving dataset: {e}\")\n",
    "\n",
    "# Display final results\n",
    "print(\"\\nFinal dataset preview (requested format):\")\n",
    "print(final_df.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"Total reviews: {len(final_df)}\")\n",
    "sentiment_counts = final_df['sentimenLabel'].value_counts()\n",
    "print(f\"Sentiment distribution: {dict(sentiment_counts)}\")\n",
    "\n",
    "# Calculate and display processing time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
